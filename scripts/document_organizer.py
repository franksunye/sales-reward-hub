#!/usr/bin/env python3
"""
æ–‡æ¡£æ•´ç†å·¥å…·

åˆ†æå’Œæ•´ç†é¡¹ç›®æ–‡æ¡£ï¼Œè¯†åˆ«è¿‡æ—¶ã€é‡å¤æˆ–æ— ç”¨çš„æ–‡æ¡£ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/document_organizer.py
    python scripts/document_organizer.py --output reports/document_analysis.md
"""

import sys
import os
from typing import Dict, List, Set
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

class DocumentOrganizer:
    """æ–‡æ¡£æ•´ç†å™¨"""
    
    def __init__(self):
        self.project_root = Path(project_root)
        self.docs_dir = self.project_root / 'docs'
        self.analysis_result = {
            'timestamp': datetime.now().isoformat(),
            'total_docs': 0,
            'categories': {},
            'duplicates': [],
            'outdated': [],
            'core_docs': [],
            'recommendations': []
        }
    
    def analyze_all(self):
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ“š å¼€å§‹æ–‡æ¡£åˆ†æ...")
        
        if not self.docs_dir.exists():
            print("âŒ docsç›®å½•ä¸å­˜åœ¨")
            return
        
        self.categorize_documents()
        self.identify_duplicates()
        self.identify_outdated()
        self.identify_core_docs()
        self.generate_recommendations()
        
        print("âœ… æ–‡æ¡£åˆ†æå®Œæˆ")
    
    def categorize_documents(self):
        """åˆ†ç±»æ–‡æ¡£"""
        print("ğŸ“‹ åˆ†ç±»æ–‡æ¡£...")
        
        categories = {
            'validation': [],
            'planning': [],
            'status': [],
            'guide': [],
            'architecture': [],
            'testing': [],
            'deployment': [],
            'business': [],
            'other': []
        }
        
        # éå†æ‰€æœ‰markdownæ–‡ä»¶
        for doc_file in self.docs_dir.rglob("*.md"):
            if doc_file.is_file():
                doc_name = doc_file.name.lower()
                relative_path = str(doc_file.relative_to(self.project_root))
                
                # æ ¹æ®æ–‡ä»¶ååˆ†ç±»
                if any(keyword in doc_name for keyword in ['valid', 'verify', 'check', 'test']):
                    categories['validation'].append(relative_path)
                elif any(keyword in doc_name for keyword in ['plan', 'roadmap', 'phase']):
                    categories['planning'].append(relative_path)
                elif any(keyword in doc_name for keyword in ['status', 'update', 'report']):
                    categories['status'].append(relative_path)
                elif any(keyword in doc_name for keyword in ['guide', 'how', 'instruction']):
                    categories['guide'].append(relative_path)
                elif any(keyword in doc_name for keyword in ['architecture', 'design', 'structure']):
                    categories['architecture'].append(relative_path)
                elif any(keyword in doc_name for keyword in ['test', 'spec']):
                    categories['testing'].append(relative_path)
                elif any(keyword in doc_name for keyword in ['deploy', 'install', 'setup']):
                    categories['deployment'].append(relative_path)
                elif any(keyword in doc_name for keyword in ['business', 'rule', 'config']):
                    categories['business'].append(relative_path)
                else:
                    categories['other'].append(relative_path)
        
        self.analysis_result['categories'] = categories
        self.analysis_result['total_docs'] = sum(len(docs) for docs in categories.values())
    
    def identify_duplicates(self):
        """è¯†åˆ«é‡å¤æ–‡æ¡£"""
        print("ğŸ” è¯†åˆ«é‡å¤æ–‡æ¡£...")
        
        # æŒ‰ä¸»é¢˜åˆ†ç»„
        topic_groups = {
            'beijing_validation': [],
            'shanghai_validation': [],
            'integration_test': [],
            'phase_plan': [],
            'status_report': []
        }
        
        all_docs = []
        for category_docs in self.analysis_result['categories'].values():
            all_docs.extend(category_docs)
        
        for doc_path in all_docs:
            doc_name = Path(doc_path).name.lower()
            
            if 'beijing' in doc_name and 'valid' in doc_name:
                topic_groups['beijing_validation'].append(doc_path)
            elif 'shanghai' in doc_name and 'valid' in doc_name:
                topic_groups['shanghai_validation'].append(doc_path)
            elif 'integration' in doc_name and 'test' in doc_name:
                topic_groups['integration_test'].append(doc_path)
            elif 'phase' in doc_name and 'plan' in doc_name:
                topic_groups['phase_plan'].append(doc_path)
            elif 'status' in doc_name or 'report' in doc_name:
                topic_groups['status_report'].append(doc_path)
        
        # è¯†åˆ«å¯èƒ½é‡å¤çš„ç»„
        for topic, docs in topic_groups.items():
            if len(docs) > 1:
                self.analysis_result['duplicates'].append({
                    'topic': topic,
                    'documents': docs,
                    'count': len(docs)
                })
    
    def identify_outdated(self):
        """è¯†åˆ«è¿‡æ—¶æ–‡æ¡£"""
        print("ğŸ“… è¯†åˆ«è¿‡æ—¶æ–‡æ¡£...")
        
        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´å’Œå†…å®¹
        outdated_indicators = [
            'TODO',
            'å¾…å®Œæˆ',
            'è®¡åˆ’ä¸­',
            'å‡†å¤‡ä¸­',
            'v0.',
            'è‰ç¨¿',
            'draft'
        ]
        
        for category_docs in self.analysis_result['categories'].values():
            for doc_path in category_docs:
                full_path = self.project_root / doc_path
                
                try:
                    # æ£€æŸ¥æ–‡ä»¶å†…å®¹
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æ£€æŸ¥è¿‡æ—¶æŒ‡æ ‡
                    outdated_score = 0
                    found_indicators = []
                    
                    for indicator in outdated_indicators:
                        if indicator in content:
                            outdated_score += 1
                            found_indicators.append(indicator)
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    if len(content) < 500:  # å†…å®¹å¤ªå°‘å¯èƒ½æ˜¯è‰ç¨¿
                        outdated_score += 1
                        found_indicators.append('å†…å®¹è¿‡å°‘')
                    
                    # å¦‚æœè¿‡æ—¶æŒ‡æ ‡è¾ƒå¤šï¼Œæ ‡è®°ä¸ºè¿‡æ—¶
                    if outdated_score >= 2:
                        self.analysis_result['outdated'].append({
                            'document': doc_path,
                            'score': outdated_score,
                            'indicators': found_indicators,
                            'size': len(content)
                        })
                        
                except Exception as e:
                    print(f"âš ï¸  æ— æ³•åˆ†ææ–‡æ¡£ {doc_path}: {e}")
    
    def identify_core_docs(self):
        """è¯†åˆ«æ ¸å¿ƒæ–‡æ¡£"""
        print("â­ è¯†åˆ«æ ¸å¿ƒæ–‡æ¡£...")
        
        # æ ¸å¿ƒæ–‡æ¡£çš„ç‰¹å¾
        core_indicators = [
            ('README.md', 10),
            ('architecture', 8),
            ('guide', 7),
            ('plan', 6),
            ('config', 6),
            ('deployment', 7),
            ('validation', 5)
        ]
        
        for category_docs in self.analysis_result['categories'].values():
            for doc_path in category_docs:
                doc_name = Path(doc_path).name.lower()
                
                score = 0
                reasons = []
                
                # è®¡ç®—æ ¸å¿ƒæ–‡æ¡£åˆ†æ•°
                for indicator, weight in core_indicators:
                    if indicator in doc_name or indicator in doc_path.lower():
                        score += weight
                        reasons.append(f"{indicator}({weight})")
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆå†…å®¹ä¸°å¯Œçš„æ–‡æ¡£æ›´é‡è¦ï¼‰
                try:
                    full_path = self.project_root / doc_path
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    if len(content) > 2000:  # å†…å®¹ä¸°å¯Œ
                        score += 3
                        reasons.append("å†…å®¹ä¸°å¯Œ(3)")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æ„åŒ–å†…å®¹
                    if content.count('#') > 5:  # æœ‰å¤šä¸ªæ ‡é¢˜
                        score += 2
                        reasons.append("ç»“æ„åŒ–(2)")
                        
                except Exception:
                    pass
                
                # æ ¸å¿ƒæ–‡æ¡£é˜ˆå€¼
                if score >= 8:
                    self.analysis_result['core_docs'].append({
                        'document': doc_path,
                        'score': score,
                        'reasons': reasons
                    })
    
    def generate_recommendations(self):
        """ç”Ÿæˆæ•´ç†å»ºè®®"""
        recommendations = []
        
        # é‡å¤æ–‡æ¡£å»ºè®®
        if self.analysis_result['duplicates']:
            recommendations.append({
                'category': 'é‡å¤æ–‡æ¡£',
                'priority': 'high',
                'description': f"å‘ç° {len(self.analysis_result['duplicates'])} ç»„é‡å¤ä¸»é¢˜æ–‡æ¡£",
                'action': 'åˆå¹¶ç›¸åŒä¸»é¢˜çš„æ–‡æ¡£ï¼Œä¿ç•™æœ€å®Œæ•´å’Œæœ€æ–°çš„ç‰ˆæœ¬'
            })
        
        # è¿‡æ—¶æ–‡æ¡£å»ºè®®
        if self.analysis_result['outdated']:
            recommendations.append({
                'category': 'è¿‡æ—¶æ–‡æ¡£',
                'priority': 'medium',
                'description': f"å‘ç° {len(self.analysis_result['outdated'])} ä¸ªå¯èƒ½è¿‡æ—¶çš„æ–‡æ¡£",
                'action': 'æ›´æ–°æˆ–åˆ é™¤è¿‡æ—¶æ–‡æ¡£ï¼Œç¡®ä¿æ–‡æ¡£çš„æ—¶æ•ˆæ€§'
            })
        
        # æ–‡æ¡£ç»“æ„å»ºè®®
        total_docs = self.analysis_result['total_docs']
        if total_docs > 20:
            recommendations.append({
                'category': 'æ–‡æ¡£æ•°é‡',
                'priority': 'medium',
                'description': f"æ–‡æ¡£æ€»æ•° {total_docs} ä¸ªï¼Œå¯èƒ½è¿‡å¤š",
                'action': 'ç²¾ç®€æ–‡æ¡£æ•°é‡ï¼Œä¿ç•™æ ¸å¿ƒæ–‡æ¡£ï¼Œå½’æ¡£å†å²æ–‡æ¡£'
            })
        
        # æ ¸å¿ƒæ–‡æ¡£å»ºè®®
        core_count = len(self.analysis_result['core_docs'])
        recommendations.append({
            'category': 'æ ¸å¿ƒæ–‡æ¡£',
            'priority': 'low',
            'description': f"è¯†åˆ«å‡º {core_count} ä¸ªæ ¸å¿ƒæ–‡æ¡£",
            'action': 'ç¡®ä¿æ ¸å¿ƒæ–‡æ¡£ä¿æŒæ›´æ–°ï¼Œä½œä¸ºé¡¹ç›®çš„ä¸»è¦æ–‡æ¡£'
        })
        
        self.analysis_result['recommendations'] = recommendations
    
    def generate_report(self, output_file: str = None):
        """ç”Ÿæˆæ•´ç†æŠ¥å‘Š"""
        report_lines = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        report_lines.append("# æ–‡æ¡£æ•´ç†åˆ†ææŠ¥å‘Š")
        report_lines.append("")
        report_lines.append(f"**åˆ†ææ—¶é—´**: {self.analysis_result['timestamp']}")
        report_lines.append(f"**æ–‡æ¡£æ€»æ•°**: {self.analysis_result['total_docs']}")
        report_lines.append("")
        
        # æ–‡æ¡£åˆ†ç±»
        report_lines.append("## ğŸ“š æ–‡æ¡£åˆ†ç±»")
        for category, docs in self.analysis_result['categories'].items():
            if docs:
                report_lines.append(f"### {category.title()} ({len(docs)} ä¸ª)")
                for doc in sorted(docs):
                    report_lines.append(f"- {doc}")
                report_lines.append("")
        
        # é‡å¤æ–‡æ¡£
        if self.analysis_result['duplicates']:
            report_lines.append("## ğŸ”„ é‡å¤æ–‡æ¡£")
            for dup in self.analysis_result['duplicates']:
                report_lines.append(f"### {dup['topic']} ({dup['count']} ä¸ª)")
                for doc in dup['documents']:
                    report_lines.append(f"- {doc}")
                report_lines.append("")
        
        # è¿‡æ—¶æ–‡æ¡£
        if self.analysis_result['outdated']:
            report_lines.append("## ğŸ“… å¯èƒ½è¿‡æ—¶çš„æ–‡æ¡£")
            for outdated in self.analysis_result['outdated']:
                report_lines.append(f"### {outdated['document']}")
                report_lines.append(f"- **è¿‡æ—¶åˆ†æ•°**: {outdated['score']}")
                report_lines.append(f"- **æŒ‡æ ‡**: {', '.join(outdated['indicators'])}")
                report_lines.append(f"- **æ–‡ä»¶å¤§å°**: {outdated['size']} å­—ç¬¦")
                report_lines.append("")
        
        # æ ¸å¿ƒæ–‡æ¡£
        if self.analysis_result['core_docs']:
            report_lines.append("## â­ æ ¸å¿ƒæ–‡æ¡£")
            # æŒ‰åˆ†æ•°æ’åº
            sorted_core = sorted(self.analysis_result['core_docs'], 
                               key=lambda x: x['score'], reverse=True)
            for core in sorted_core:
                report_lines.append(f"### {core['document']} (åˆ†æ•°: {core['score']})")
                report_lines.append(f"- **è¯„åˆ†åŸå› **: {', '.join(core['reasons'])}")
                report_lines.append("")
        
        # æ•´ç†å»ºè®®
        if self.analysis_result['recommendations']:
            report_lines.append("## ğŸ’¡ æ•´ç†å»ºè®®")
            for rec in self.analysis_result['recommendations']:
                priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}
                emoji = priority_emoji.get(rec['priority'], 'âšª')
                report_lines.append(f"### {emoji} {rec['category']} ({rec['priority']})")
                report_lines.append(f"**é—®é¢˜**: {rec['description']}")
                report_lines.append(f"**å»ºè®®**: {rec['action']}")
                report_lines.append("")
        
        # æ¨èçš„æ–‡æ¡£ç»“æ„
        report_lines.append("## ğŸ“ æ¨èçš„æ–‡æ¡£ç»“æ„")
        report_lines.append("```")
        report_lines.append("docs/")
        report_lines.append("â”œâ”€â”€ README.md                    # é¡¹ç›®æ¦‚è¿°")
        report_lines.append("â”œâ”€â”€ architecture.md              # ç³»ç»Ÿæ¶æ„")
        report_lines.append("â”œâ”€â”€ deployment_guide.md          # éƒ¨ç½²æŒ‡å—")
        report_lines.append("â”œâ”€â”€ validation_guide.md          # éªŒè¯æŒ‡å—")
        report_lines.append("â”œâ”€â”€ business_rules.md            # ä¸šåŠ¡è§„åˆ™")
        report_lines.append("â”œâ”€â”€ current_status.md            # å½“å‰çŠ¶æ€")
        report_lines.append("â”œâ”€â”€ archived/                    # å½’æ¡£æ–‡æ¡£")
        report_lines.append("â”‚   â”œâ”€â”€ historical_reports/")
        report_lines.append("â”‚   â””â”€â”€ old_plans/")
        report_lines.append("â””â”€â”€ reports/                     # åˆ†ææŠ¥å‘Š")
        report_lines.append("```")
        
        report_content = "\n".join(report_lines)
        
        # è¾“å‡ºæŠ¥å‘Š
        if output_file:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        else:
            print(report_content)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ–‡æ¡£æ•´ç†å·¥å…·')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    
    organizer = DocumentOrganizer()
    organizer.analyze_all()
    
    output_file = args.output or f"reports/document_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    organizer.generate_report(output_file)

if __name__ == "__main__":
    main()
