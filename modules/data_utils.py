# data_utils.py - æ•°æ®å¤„ç†å·¥å…·æ¨¡å—
import csv
import logging
import pandas as pd
import os
import shutil
import json
from datetime import datetime, timezone
import pandas as pd
import pytz
import re
from modules.log_config import setup_logging

# è®¾ç½®æ—¥å¿—
setup_logging()

def save_to_csv_with_headers(data, filename='ContractData.csv', columns=None):
    if columns is None:
        columns = ["åˆåŒID(_id)", "æ´»åŠ¨åŸå¸‚(province)", "å·¥å•ç¼–å·(serviceAppointmentNum)", "Status", "ç®¡å®¶(serviceHousekeeper)", "åˆåŒç¼–å·(contractdocNum)", "åˆåŒé‡‘é¢(adjustRefundMoney)", "æ”¯ä»˜é‡‘é¢(paidAmount)", "å·®é¢(difference)", "State", "åˆ›å»ºæ—¶é—´(createTime)", "æœåŠ¡å•†(orgName)", "ç­¾çº¦æ—¶é—´(signedDate)", "Doorsill", "æ¬¾é¡¹æ¥æºç±»å‹(tradeIn)"]
    
    df = pd.DataFrame(data, columns=columns)   
    df.to_csv(filename, index=False)

def archive_file(filename, archive_dir='archive', days_to_keep=1):
    # Get current timestamp in China timezone
    china_tz = pytz.timezone('Asia/Shanghai')
    timestamp = datetime.now(china_tz).strftime('%Y%m%d%H%M')

    # Define archive file name
    base_name = os.path.splitext(filename)[0]
    ext = os.path.splitext(filename)[1]
    archive_file_name = f'{base_name}_{timestamp}{ext}'

    # Create archive directory if it doesn't exist
    if not os.path.exists(archive_dir):
        os.makedirs(archive_dir)

    # If the filename includes a path, ensure the directory structure exists in the archive
    dir_path = os.path.dirname(filename)
    if dir_path:
        archive_subdir = os.path.join(archive_dir, dir_path)
        if not os.path.exists(archive_subdir):
            os.makedirs(archive_subdir)

    # Move file to archive directory
    shutil.move(filename, os.path.join(archive_dir, archive_file_name))
    
    # Check and delete files in the archive directory that are older than the specified number of days
    for root, _, files in os.walk(archive_dir):
        for file in files:
            file_path = os.path.join(root, file)
            if os.path.isfile(file_path):
                file_modified_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                if (datetime.now() - file_modified_time).days > days_to_keep:
                    os.remove(file_path)
                    logging.debug(f"Deleted old file: {file_path}")
                    
def read_contract_data(filename):
    with open(filename, 'r', encoding='utf-8-sig') as file:
        reader = csv.DictReader(file)
        return list(reader)
    
def read_daily_service_report(filename):
    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file)
        return list(reader)

def get_all_records_from_csv(filename):
    """è¯»å–æ€§èƒ½æ•°æ®æ–‡ä»¶å¹¶è¿”å›è®°å½•åˆ—è¡¨"""
    with open(filename, mode='r', encoding='utf-8-sig', newline='') as file:
        reader = csv.DictReader(file)
        return list(reader)
    
def collect_unique_contract_ids_from_file(filename):
    try:
        existing_contract_ids = set()
        with open(filename, 'r', encoding='utf-8-sig') as file:
            reader = csv.DictReader(file)
            for row in reader:
                existing_contract_ids.add(row['åˆåŒID(_id)'].strip())
        return existing_contract_ids
    except FileNotFoundError:
        return set()
    

def write_performance_data(filename, data, headers):
    with open(filename, 'a', newline='', encoding='utf-8-sig') as file:  # æ³¨æ„è¿™é‡Œæ”¹ä¸ºè¿½åŠ æ¨¡å¼ 'a'
        writer = csv.DictWriter(file, fieldnames=headers)
        if file.tell() == 0:  # å¦‚æœæ–‡ä»¶æ˜¯ç©ºçš„ï¼Œå†™å…¥å¤´éƒ¨
            writer.writeheader()
        writer.writerows(data)
        
def write_performance_data_to_csv(filename, data, fieldnames):
    """å†™å…¥æ€§èƒ½æ•°æ®åˆ°æ–‡ä»¶"""
    with open(filename, mode='w', encoding='utf-8-sig', newline='') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data)
        
def get_housekeeper_award_list(file_path):

    try:
        # Load the CSV file
        data = pd.read_csv(file_path)
        
        # Group by 'ç®¡å®¶(serviceHousekeeper)' and aggregate 'å¥–åŠ±åç§°' into a list
        grouped_rewards = data.groupby('ç®¡å®¶(serviceHousekeeper)')['å¥–åŠ±åç§°'].apply(list).to_dict()
        
        # Clean: Remove NaN values, duplicates, and split combined rewards
        cleaned_grouped_rewards = {}
        for housekeeper, rewards in grouped_rewards.items():
            cleaned_rewards = []
            for reward in filter(pd.notna, rewards):
                # Split combined rewards and extend the list
                cleaned_rewards.extend(reward.split(", "))
            # Remove duplicates
            cleaned_grouped_rewards[housekeeper] = list(dict.fromkeys(cleaned_rewards))
        
        return cleaned_grouped_rewards
    except FileNotFoundError:
        return []

# é‡å†™ï¼Œè·å–å”¯ä¸€çš„ç®¡å®¶å¥–åŠ±åˆ—è¡¨
def get_unique_housekeeper_award_list(file_path):

    try:
        # Load the CSV file
        data = pd.read_csv(file_path)

        if data.empty:
            return {}  # å¤„ç†ç©º DataFrame çš„æƒ…å†µ
        
        # Construct a new column that combines 'ç®¡å®¶(serviceHousekeeper)' and 'æœåŠ¡å•†(orgName)'
        data['unique_key'] = data.apply(lambda row: f"{row['ç®¡å®¶(serviceHousekeeper)']}_{row['æœåŠ¡å•†(orgName)']}", axis=1)
        
        # Group by the constructed key and aggregate 'å¥–åŠ±åç§°' into a list
        grouped_rewards = data.groupby('unique_key')['å¥–åŠ±åç§°'].apply(list).to_dict()

        # Clean: Remove NaN values, duplicates, and split combined rewards
        cleaned_grouped_rewards = {}
        for housekeeper, rewards in grouped_rewards.items():
            cleaned_rewards = []
            for reward in filter(pd.notna, rewards):
                # Split combined rewards and extend the list
                cleaned_rewards.extend(reward.split(", "))
            # Remove duplicates
            cleaned_grouped_rewards[housekeeper] = list(dict.fromkeys(cleaned_rewards))
        
        return cleaned_grouped_rewards
    except FileNotFoundError:
        return {}
    except pd.errors.EmptyDataError:
        return {}  # å¤„ç†ç©ºæ–‡ä»¶çš„æƒ…å†µ
    
def load_send_status(filename):
    """åŠ è½½å‘é€çŠ¶æ€æ–‡ä»¶"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            if not content:
                return {}  # å¤„ç†ç©ºæ–‡ä»¶
            return json.loads(content)
    except FileNotFoundError:
        return {}
    except json.JSONDecodeError:
        return {}  # å¤„ç†æ— æ•ˆJSONæ–‡ä»¶

def save_send_status(filename, status):
    """ä¿å­˜å‘é€çŠ¶æ€åˆ°æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(status, f, ensure_ascii=False, indent=4)

def update_send_status(filename, _id, status):
    """æ›´æ–°æŒ‡å®šåˆåŒIDçš„å‘é€çŠ¶æ€"""
    logging.info(f"Starting update_send_status for _id: {_id}, status: {status}")

    send_status = load_send_status(filename)
    send_status[_id] = status

    logging.info(f"Updating send_status for _id: {_id} to status: {status}")

    save_send_status(filename, send_status)
    logging.info(f"Successfully updated send_status for _id: {_id} to status: {status}")


# ==================== æ—¶é—´å¤„ç†ç›¸å…³å‡½æ•° ====================

def format_create_time(iso_time_str):
    """å°†ISOæ—¶é—´æ ¼å¼è½¬æ¢ä¸ºæ˜“è¯»æ ¼å¼"""
    try:
        # å¤„ç†å¸¦æ—¶åŒºçš„ISOæ ¼å¼
        if '+' in iso_time_str:
            dt = datetime.fromisoformat(iso_time_str)
        else:
            dt = datetime.fromisoformat(iso_time_str.replace('Z', '+00:00'))
        return dt.strftime('%Y-%m-%d %H:%M')
    except Exception as e:
        logging.warning(f'æ—¶é—´æ ¼å¼è½¬æ¢å¤±è´¥: {iso_time_str}, é”™è¯¯: {e}')
        return iso_time_str

def format_simple_date(create_time_str):
    """æ ¼å¼åŒ–åˆ›å»ºæ—¶é—´ä¸ºç®€å•çš„æœˆ-æ—¥æ ¼å¼"""
    try:
        # è§£æåˆ›å»ºæ—¶é—´
        if '+' in create_time_str:
            create_time = datetime.fromisoformat(create_time_str)
        else:
            create_time = datetime.fromisoformat(create_time_str.replace('Z', '+00:00'))

        # æ ¼å¼åŒ–ä¸ºMM-DD
        return f"{create_time.month:02d}-{create_time.day:02d}"

    except Exception as e:
        logging.warning(f"æ—¶é—´æ ¼å¼åŒ–å¤±è´¥: {create_time_str}, é”™è¯¯: {e}")
        return "æœªçŸ¥"

def calculate_retention_duration(create_time_str):
    """è®¡ç®—å·¥å•æ»ç•™æ—¶é•¿"""
    try:
        # è§£æåˆ›å»ºæ—¶é—´
        if '+' in create_time_str:
            create_time = datetime.fromisoformat(create_time_str)
        else:
            create_time = datetime.fromisoformat(create_time_str.replace('Z', '+00:00'))

        # è·å–å½“å‰æ—¶é—´ï¼ˆå¸¦æ—¶åŒºï¼‰
        current_time = datetime.now(timezone.utc)

        # ç¡®ä¿åˆ›å»ºæ—¶é—´ä¹Ÿæœ‰æ—¶åŒºä¿¡æ¯
        if create_time.tzinfo is None:
            create_time = create_time.replace(tzinfo=timezone.utc)

        # è®¡ç®—æ—¶é—´å·®
        duration = current_time - create_time

        # æ ¼å¼åŒ–æ˜¾ç¤ºï¼ˆç®€åŒ–ä¸ºå¤©æ•°é¢—ç²’åº¦ï¼‰
        days = int(duration.total_seconds() // (24 * 3600))
        return f"{days}å¤©"

    except Exception as e:
        logging.warning(f"æ»ç•™æ—¶é•¿è®¡ç®—å¤±è´¥: {create_time_str}, é”™è¯¯: {e}")
        return "æœªçŸ¥"


# ==================== æ•°æ®å¤„ç†ç›¸å…³å‡½æ•° ====================

def filter_orders_by_time_threshold(orders_data):
    """
    è¿‡æ»¤å·¥å•æ•°æ®ï¼Œæ’é™¤ï¼š
    - å¾…é¢„çº¦çŠ¶æ€ï¼Œ48å°æ—¶ä¹‹å†…çš„éœ€è¦æ’é™¤
    - æš‚ä¸ä¸Šé—¨çŠ¶æ€ï¼Œ48å°æ—¶ä¹‹å†…çš„éœ€è¦æ’é™¤

    Args:
        orders_data: åŸå§‹å·¥å•æ•°æ®åˆ—è¡¨

    Returns:
        filtered_orders: è¿‡æ»¤åçš„å·¥å•æ•°æ®åˆ—è¡¨
    """
    filtered_orders = []
    current_time = datetime.now(timezone.utc)

    for order in orders_data:
        try:
            # è§£æå·¥å•æ•°æ®
            order_info = {
                'orderNum': order[0],
                'name': order[1],
                'address': order[2],
                'supervisorName': order[3],
                'createTime': order[4],
                'orgName': order[5],
                'orderstatus': order[6]
            }

            # è§£æåˆ›å»ºæ—¶é—´
            create_time_str = order_info['createTime']
            if '+' in create_time_str:
                create_time = datetime.fromisoformat(create_time_str)
            else:
                create_time = datetime.fromisoformat(create_time_str.replace('Z', '+00:00'))

            # ç¡®ä¿åˆ›å»ºæ—¶é—´æœ‰æ—¶åŒºä¿¡æ¯
            if create_time.tzinfo is None:
                create_time = create_time.replace(tzinfo=timezone.utc)

            # è®¡ç®—æ—¶é—´å·®ï¼ˆå°æ—¶ï¼‰
            time_diff = current_time - create_time
            hours_elapsed = time_diff.total_seconds() / 3600

            # è·å–å·¥å•çŠ¶æ€
            order_status = order_info['orderstatus']

            # è¿‡æ»¤é€»è¾‘ï¼šå¾…é¢„çº¦å’Œæš‚ä¸ä¸Šé—¨çŠ¶æ€ï¼Œ48å°æ—¶ä¹‹å†…çš„éœ€è¦æ’é™¤
            should_include = True

            if ('å¾…é¢„çº¦' in order_status or 'æš‚ä¸ä¸Šé—¨' in order_status) and hours_elapsed < 48:
                should_include = False
                logging.info(f"è¿‡æ»¤æ‰å·¥å• {order_info['orderNum']} (çŠ¶æ€: {order_status}, åˆ›å»ºæ—¶é—´: {hours_elapsed:.1f}å°æ—¶å‰)")

            if should_include:
                filtered_orders.append(order)

        except Exception as e:
            logging.warning(f"å¤„ç†å·¥å•æ•°æ®æ—¶å‡ºé”™ï¼Œè·³è¿‡: {order}, é”™è¯¯: {e}")
            continue

    return filtered_orders

def group_orders_by_org(orders_data):
    """æŒ‰æœåŠ¡å•†åˆ†ç»„å·¥å•æ•°æ®"""
    grouped = {}

    for order in orders_data:
        try:
            # æ ¹æ®APIæµ‹è¯•ç»“æœï¼Œå­—æ®µç´¢å¼•æ˜ å°„
            order_info = {
                'orderNum': order[0],
                'name': order[1],
                'address': order[2],
                'supervisorName': order[3],
                'createTime': order[4],
                'orgName': order[5],
                'orderstatus': order[6]
            }

            org_name = order_info['orgName']
            if org_name not in grouped:
                grouped[org_name] = []
            grouped[org_name].append(order_info)

        except (IndexError, KeyError) as e:
            logging.warning(f'å·¥å•æ•°æ®æ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡: {order}, é”™è¯¯: {e}')
            continue

    return grouped

def simplify_order_number(order_num):
    """ç®€åŒ–å·¥å•å·ï¼Œåªä¿ç•™å5ä½æ•°å­—"""
    if not order_num:
        return "-"

    # æå–æ•°å­—éƒ¨åˆ†
    numbers = re.findall(r'\d+', order_num)
    if numbers:
        # å–æœ€åä¸€ä¸ªæ•°å­—ä¸²çš„å5ä½
        last_number = numbers[-1]
        if len(last_number) >= 5:
            return last_number[-5:]
        else:
            return last_number
    return order_num


# ==================== æ¶ˆæ¯æ ¼å¼åŒ–ç›¸å…³å‡½æ•° ====================

def format_pending_orders_message_text(org_name, orders):
    """æ ¼å¼åŒ–å·¥å•æé†’æ¶ˆæ¯ï¼ˆæ–‡æœ¬æ ¼å¼ï¼Œä¿ç•™ä½œä¸ºå¤‡ç”¨ï¼‰"""
    count = len(orders)

    message_lines = [
        f"ğŸ“‹ å¾…é¢„çº¦å·¥å•æé†’ ({org_name})",
        "",
        f"å…±æœ‰ {count} ä¸ªå·¥å•å¾…é¢„çº¦ï¼š",
        ""
    ]

    for i, order in enumerate(orders, 1):
        # ä½¿ç”¨æ–°çš„ç®€åŒ–æ ¼å¼
        simple_date = format_simple_date(order['createTime'])
        retention_duration = calculate_retention_duration(order['createTime'])
        simple_order_num = simplify_order_number(order['orderNum'])

        order_text = f"""{i:02d}. å·¥å•å·ï¼š{simple_order_num}
     å®¢æˆ·ï¼š{order['name']}
     åœ°å€ï¼š{order['address']}
     è´Ÿè´£äººï¼š{order['supervisorName']}
     åˆ›å»ºæ—¶é—´ï¼š{simple_date}ï¼ˆ{retention_duration}ï¼‰
     çŠ¶æ€ï¼š{order['orderstatus']}"""

        message_lines.append(order_text)
        if i < count:  # ä¸æ˜¯æœ€åä¸€ä¸ªï¼Œæ·»åŠ ç©ºè¡Œ
            message_lines.append("")

    message_lines.extend([
        "",
        "è¯·åŠæ—¶è·Ÿè¿›å¤„ç†ï¼Œå¦‚æœ‰ç–‘é—®è¯·è”ç³»è¿è¥äººå‘˜ã€‚"
    ])

    return "\n".join(message_lines)

def format_pending_orders_message(org_name, orders):
    """æ ¼å¼åŒ–å·¥å•æé†’æ¶ˆæ¯ï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰"""
    count = len(orders)

    # æ¶ˆæ¯å¤´éƒ¨
    header = f"ğŸ“‹ **å¾…é¢„çº¦å·¥å•æé†’** ({org_name})\n\nå…±æœ‰ **{count}** ä¸ªå·¥å•å¾…é¢„çº¦ï¼š\n"

    # è¡¨æ ¼å¤´éƒ¨ï¼ˆå¢åŠ æ»ç•™æ—¶é•¿åˆ—ï¼‰
    table_header = """
| åºå· | å·¥å•å· | æ»ç•™æ—¶é•¿ | å®¢æˆ· | åœ°å€ | ç®¡å®¶ | åˆ›å»ºæ—¶é—´ | çŠ¶æ€ |
| --- | --- | --- | --- | --- | --- | --- | --- |"""

    # è¡¨æ ¼å†…å®¹
    table_rows = []
    for i, order in enumerate(orders, 1):
        # ä½¿ç”¨æ–°çš„ç®€åŒ–æ ¼å¼
        simple_date = format_simple_date(order['createTime'])
        retention_duration = calculate_retention_duration(order['createTime'])
        simple_order_num = simplify_order_number(order['orderNum'])

        # å¤„ç†å¯èƒ½åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„å­—æ®µ
        customer = order['name'].replace('|', '\\|') if order['name'] else '-'
        address = order['address'].replace('|', '\\|') if order['address'] else '-'
        supervisor = order['supervisorName'].replace('|', '\\|') if order['supervisorName'] else '-'
        status = order['orderstatus'].replace('|', '\\|') if order['orderstatus'] else '-'

        row = f"| {i:02d} | {simple_order_num} | {retention_duration} | {customer} | {address} | {supervisor} | {simple_date} | {status} |"
        table_rows.append(row)

    # æ¶ˆæ¯åº•éƒ¨
    footer = "\n\nè¯·åŠæ—¶è·Ÿè¿›å¤„ç†ï¼Œå¦‚æœ‰ç–‘é—®è¯·è”ç³»è¿è¥äººå‘˜ã€‚"

    # ç»„åˆå®Œæ•´æ¶ˆæ¯
    message = header + table_header + "\n" + "\n".join(table_rows) + footer

    return message