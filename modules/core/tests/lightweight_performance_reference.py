"""
é”€å”®æ¿€åŠ±ç³»ç»Ÿé‡æ„ - è½»é‡åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•
ç‰ˆæœ¬: v1.0
åˆ›å»ºæ—¥æœŸ: 2025-01-08

æ³¨æ„ï¼šæ€§èƒ½ä¸æ˜¯é‡ç‚¹ï¼æ•°æ®é‡å°ï¼Œæ­¤æµ‹è¯•ä»…ä½œä¸ºå‚è€ƒ
é‡ç‚¹ï¼šåŠŸèƒ½æ­£ç¡®æ€§ï¼Œæ€§èƒ½æµ‹è¯•åªæ˜¯ä¸ºäº†ç¡®è®¤æ²¡æœ‰æ˜æ˜¾çš„æ€§èƒ½é€€åŒ–
"""

import time
import tempfile
import os
import sys
import statistics
from typing import List, Dict, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from modules.core import create_standard_pipeline


class LightweightPerformanceReference:
    """è½»é‡åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯• - ä»…ä½œå‚è€ƒï¼Œä¸æ˜¯é‡ç‚¹"""
    
    def __init__(self):
        self.results = []
    
    def create_sample_data(self, count: int = 10) -> List[Dict]:
        """åˆ›å»ºæ ·æœ¬æ•°æ®ï¼ˆå°æ•°æ®é‡ï¼‰"""
        sample_data = []
        
        for i in range(count):
            sample_data.append({
                'åˆåŒID(_id)': f'2025010812345{i:03d}',
                'ç®¡å®¶(serviceHousekeeper)': f'æµ‹è¯•ç®¡å®¶{i%3+1}',
                'æœåŠ¡å•†(orgName)': 'æµ‹è¯•æœåŠ¡å•†',
                'åˆåŒé‡‘é¢(adjustRefundMoney)': 10000 + (i * 1000),
                'æ”¯ä»˜é‡‘é¢(paidAmount)': 8000 + (i * 800),
                'æ¬¾é¡¹æ¥æºç±»å‹(tradeIn)': i % 2,
                'ç®¡å®¶ID(serviceHousekeeperId)': f'TEST{i:03d}',
                'æ´»åŠ¨åŸå¸‚(province)': 'åŒ—äº¬' if i % 2 == 0 else 'ä¸Šæµ·',
                'Status': 'å·²ç­¾çº¦',
                'åˆ›å»ºæ—¶é—´(createTime)': f'2025-01-08 {10+i%12:02d}:00:00'
            })
        
        return sample_data
    
    def measure_processing_time(self, config_key: str, activity_code: str, 
                              data: List[Dict], runs: int = 3) -> Dict[str, float]:
        """æµ‹é‡å¤„ç†æ—¶é—´ï¼ˆå¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼ï¼‰"""
        times = []
        
        for run in range(runs):
            # åˆ›å»ºä¸´æ—¶æ•°æ®åº“
            with tempfile.NamedTemporaryFile(delete=False, suffix='.db') as temp_db:
                temp_db.close()
                
                try:
                    start_time = time.time()
                    
                    # åˆ›å»ºå¤„ç†ç®¡é“
                    pipeline, config, store = create_standard_pipeline(
                        config_key=config_key,
                        activity_code=activity_code,
                        city=config_key.split('-')[0],
                        db_path=temp_db.name,
                        enable_project_limit=(config_key.startswith('BJ')),
                        enable_dual_track=(config_key == 'SH-2025-09')
                    )
                    
                    # å¤„ç†æ•°æ®
                    processed_records = pipeline.process(data)
                    
                    end_time = time.time()
                    processing_time = end_time - start_time
                    times.append(processing_time)
                    
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(temp_db.name):
                        os.unlink(temp_db.name)
        
        return {
            'average_time': statistics.mean(times),
            'min_time': min(times),
            'max_time': max(times),
            'std_dev': statistics.stdev(times) if len(times) > 1 else 0.0,
            'runs': runs,
            'records_processed': len(data)
        }
    
    def run_performance_reference(self) -> Dict[str, Dict]:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆè½»é‡åŒ–ï¼‰"""
        print("è¿è¡Œè½»é‡åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        print("æ³¨æ„ï¼šæ€§èƒ½ä¸æ˜¯é‡ç‚¹ï¼Œæ­¤æµ‹è¯•ä»…ä½œå‚è€ƒ")
        print("-" * 50)
        
        # æµ‹è¯•é…ç½®
        test_configs = [
            ('BJ-2025-06', 'BJ-JUN', 'åŒ—äº¬6æœˆ'),
            ('BJ-2025-09', 'BJ-SEP', 'åŒ—äº¬9æœˆ'),
            ('SH-2025-09', 'SH-SEP', 'ä¸Šæµ·9æœˆ')
        ]
        
        # æµ‹è¯•æ•°æ®é‡ï¼ˆå°æ•°æ®é‡ï¼‰
        data_sizes = [5, 10, 20]  # å°æ•°æ®é‡ï¼Œç¬¦åˆå®é™…ä½¿ç”¨åœºæ™¯
        
        results = {}
        
        for config_key, activity_code, name in test_configs:
            print(f"\næµ‹è¯• {name} ({config_key}):")
            results[config_key] = {}
            
            for size in data_sizes:
                print(f"  æ•°æ®é‡: {size} æ¡è®°å½•")
                
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                test_data = self.create_sample_data(size)
                
                # æµ‹é‡å¤„ç†æ—¶é—´
                timing_result = self.measure_processing_time(
                    config_key, activity_code, test_data, runs=3
                )
                
                results[config_key][size] = timing_result
                
                # è¾“å‡ºç»“æœ
                avg_time = timing_result['average_time']
                per_record = avg_time / size * 1000  # æ¯«ç§’/è®°å½•
                
                print(f"    å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.3f}ç§’")
                print(f"    æ¯æ¡è®°å½•: {per_record:.1f}æ¯«ç§’")
                print(f"    æ ‡å‡†å·®: {timing_result['std_dev']:.3f}ç§’")
        
        return results
    
    def generate_performance_report(self, results: Dict[str, Dict]) -> str:
        """ç”Ÿæˆæ€§èƒ½åŸºå‡†æŠ¥å‘Š"""
        report = f"""
è½»é‡åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š
====================
æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
é‡è¦è¯´æ˜: æ€§èƒ½ä¸æ˜¯é‡ç‚¹ï¼æ•°æ®é‡å°ï¼Œæ­¤æŠ¥å‘Šä»…ä½œå‚è€ƒ

æµ‹è¯•ç¯å¢ƒ:
- æ•°æ®é‡: 5, 10, 20 æ¡è®°å½•ï¼ˆç¬¦åˆå®é™…å°æ•°æ®é‡åœºæ™¯ï¼‰
- è¿è¡Œæ¬¡æ•°: æ¯ä¸ªæµ‹è¯•3æ¬¡å–å¹³å‡å€¼
- å­˜å‚¨: SQLiteä¸´æ—¶æ•°æ®åº“

æ€§èƒ½åŸºå‡†ç»“æœ:
"""
        
        for config_key, config_results in results.items():
            report += f"\n{config_key}:\n"
            
            for size, timing in config_results.items():
                avg_time = timing['average_time']
                per_record = avg_time / size * 1000
                
                report += f"  {size}æ¡è®°å½•: {avg_time:.3f}ç§’ ({per_record:.1f}æ¯«ç§’/è®°å½•)\n"
        
        # æ€§èƒ½è¯„ä¼°
        report += "\næ€§èƒ½è¯„ä¼°:\n"
        
        # è®¡ç®—æ€»ä½“å¹³å‡æ€§èƒ½
        all_per_record_times = []
        for config_results in results.values():
            for size, timing in config_results.items():
                per_record = timing['average_time'] / size * 1000
                all_per_record_times.append(per_record)
        
        if all_per_record_times:
            avg_per_record = statistics.mean(all_per_record_times)
            report += f"- å¹³å‡å¤„ç†é€Ÿåº¦: {avg_per_record:.1f}æ¯«ç§’/è®°å½•\n"
            
            if avg_per_record < 10:
                report += "- âœ… æ€§èƒ½ä¼˜ç§€ï¼ˆ<10æ¯«ç§’/è®°å½•ï¼‰\n"
            elif avg_per_record < 50:
                report += "- âœ… æ€§èƒ½è‰¯å¥½ï¼ˆ<50æ¯«ç§’/è®°å½•ï¼‰\n"
            elif avg_per_record < 100:
                report += "- âš ï¸ æ€§èƒ½ä¸€èˆ¬ï¼ˆ<100æ¯«ç§’/è®°å½•ï¼‰\n"
            else:
                report += "- âŒ æ€§èƒ½è¾ƒæ…¢ï¼ˆ>100æ¯«ç§’/è®°å½•ï¼‰\n"
        
        report += """
é‡è¦æé†’:
- æœ¬é¡¹ç›®æ•°æ®é‡å¾ˆå°ï¼Œæ€§èƒ½ä¸æ˜¯é‡ç‚¹å…³æ³¨é¡¹
- åŠŸèƒ½æ­£ç¡®æ€§æ˜¯é¦–è¦ç›®æ ‡
- æ­¤æ€§èƒ½æµ‹è¯•ä»…ä½œä¸ºå‚è€ƒï¼Œä¸ä½œä¸ºè¯„ä¼°æ ‡å‡†
- å®é™…ç”Ÿäº§ç¯å¢ƒæ€§èƒ½å¯èƒ½å› ç¯å¢ƒå·®å¼‚è€Œä¸åŒ
"""
        
        return report
    
    def run_comparison_with_csv_simulation(self) -> str:
        """æ¨¡æ‹Ÿä¸CSVå¤„ç†çš„æ€§èƒ½å¯¹æ¯”ï¼ˆä»…ä½œå‚è€ƒï¼‰"""
        print("\næ¨¡æ‹ŸCSV vs SQLiteæ€§èƒ½å¯¹æ¯”...")
        print("æ³¨æ„ï¼šè¿™åªæ˜¯ç†è®ºå¯¹æ¯”ï¼Œä¸æ˜¯é‡ç‚¹")
        
        # æ¨¡æ‹ŸCSVå¤„ç†æ—¶é—´ï¼ˆåŸºäºæ–‡ä»¶I/Oå¼€é”€ï¼‰
        csv_simulation = {
            5: 0.050,   # 50æ¯«ç§’ï¼ˆæ–‡ä»¶è¯»å–å¼€é”€ï¼‰
            10: 0.080,  # 80æ¯«ç§’
            20: 0.120   # 120æ¯«ç§’
        }
        
        # è¿è¡ŒSQLiteæµ‹è¯•
        sqlite_results = self.run_performance_reference()
        
        comparison_report = "\nCSV vs SQLite æ€§èƒ½å¯¹æ¯”ï¼ˆæ¨¡æ‹Ÿï¼‰:\n"
        comparison_report += "=" * 40 + "\n"
        
        for config_key, config_results in sqlite_results.items():
            comparison_report += f"\n{config_key}:\n"
            
            for size in [5, 10, 20]:
                if size in config_results:
                    sqlite_time = config_results[size]['average_time']
                    csv_time = csv_simulation[size]
                    improvement = ((csv_time - sqlite_time) / csv_time * 100)
                    
                    comparison_report += f"  {size}æ¡è®°å½•:\n"
                    comparison_report += f"    CSVæ¨¡æ‹Ÿ: {csv_time:.3f}ç§’\n"
                    comparison_report += f"    SQLite: {sqlite_time:.3f}ç§’\n"
                    comparison_report += f"    æ”¹å–„: {improvement:+.1f}%\n"
        
        comparison_report += "\næ³¨æ„ï¼šCSVæ—¶é—´ä¸ºæ¨¡æ‹Ÿå€¼ï¼Œå®é™…å¯¹æ¯”éœ€è¦çœŸå®çš„æ—§ç³»ç»Ÿæµ‹è¯•\n"
        
        return comparison_report


def main():
    """ä¸»å‡½æ•°"""
    print("é”€å”®æ¿€åŠ±ç³»ç»Ÿé‡æ„ - è½»é‡åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("é‡è¦ï¼šæ€§èƒ½ä¸æ˜¯é‡ç‚¹ï¼æ­¤æµ‹è¯•ä»…ä½œå‚è€ƒ")
    print("=" * 60)
    
    # åˆ›å»ºæ€§èƒ½æµ‹è¯•å™¨
    perf_tester = LightweightPerformanceReference()
    
    try:
        # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
        results = perf_tester.run_performance_reference()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = perf_tester.generate_performance_report(results)
        print(report)
        
        # ä¿å­˜æŠ¥å‘Š
        with open('lightweight_performance_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        # è¿è¡Œå¯¹æ¯”æµ‹è¯•
        comparison = perf_tester.run_comparison_with_csv_simulation()
        print(comparison)
        
        print("\nğŸ“‹ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: lightweight_performance_report.txt")
        print("\né‡è¦æé†’ï¼š")
        print("- æ€§èƒ½ä¸æ˜¯æœ¬é¡¹ç›®çš„é‡ç‚¹å…³æ³¨é¡¹")
        print("- åŠŸèƒ½æ­£ç¡®æ€§æ˜¯é¦–è¦ç›®æ ‡")
        print("- æ­¤æµ‹è¯•ä»…ä½œä¸ºå‚è€ƒï¼Œä¸ä½œä¸ºè¯„ä¼°æ ‡å‡†")
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("=" * 60)
    print("è½»é‡åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
