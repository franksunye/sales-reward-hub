# jobs.py
import logging
from modules.request_module import send_request_with_managed_session
from modules.data_processing_module import *
from modules.file_utils import *
from modules.notification_module import *
from modules.config import *
from modules.service_provider_sla_monitor import process_sla_violations

# 2025å¹´7æœˆï¼ŒåŒ—äº¬. 
# å¹¸è¿æ•°å­—8ï¼Œå•åˆåŒé‡‘é¢1ä¸‡ä»¥ä¸Šå’Œä»¥ä¸‹å¹¸è¿å¥–åŠ±ä¸åŒï¼›èŠ‚èŠ‚é«˜ä¸‰æ¡£ï¼›
# å•ä¸ªé¡¹ç›®ï¼ˆå·¥å•ï¼‰ç­¾çº¦åˆåŒé‡‘é¢å¤§äº5ä¸‡æ—¶ï¼Œå‚ä¸ç´¯è®¡åˆåŒé‡‘é¢è®¡ç®—æ—¶å‡æŒ‰5ä¸‡è®¡å…¥ã€‚
def signing_and_sales_incentive_july_beijing():
    contract_data_filename = TEMP_CONTRACT_DATA_FILE_BJ_JULY
    performance_data_filename = PERFORMANCE_DATA_FILENAME_BJ_JULY
    status_filename = STATUS_FILENAME_BJ_JULY
    api_url = API_URL_BJ_JULY

    logging.info('BEIJING 2025 7æœˆ, Job started ...')

    response = send_request_with_managed_session(api_url)
 
    logging.info('BEIJING 2025 7æœˆ, Request sent')

    rows = response['data']['rows']

    columns = ["åˆåŒID(_id)", "æ´»åŠ¨åŸå¸‚(province)", "å·¥å•ç¼–å·(serviceAppointmentNum)", "Status", "ç®¡å®¶(serviceHousekeeper)", "åˆåŒç¼–å·(contractdocNum)", "åˆåŒé‡‘é¢(adjustRefundMoney)", "æ”¯ä»˜é‡‘é¢(paidAmount)", "å·®é¢(difference)", "State", "åˆ›å»ºæ—¶é—´(createTime)", "æœåŠ¡å•†(orgName)", "ç­¾çº¦æ—¶é—´(signedDate)", "Doorsill", "æ¬¾é¡¹æ¥æºç±»å‹(tradeIn)", "è½¬åŒ–ç‡(conversion)", "å¹³å‡å®¢å•ä»·(average)"]
    save_to_csv_with_headers(rows,contract_data_filename,columns)

    logging.info(f'BEIJING 2025 7æœˆ, Data saved to {contract_data_filename}')

    contract_data = read_contract_data(contract_data_filename)

    existing_contract_ids = collect_unique_contract_ids_from_file(performance_data_filename)

    housekeeper_award_lists = get_housekeeper_award_list(performance_data_filename)

    # å½“æœˆçš„æ•°æ®å¤„ç†é€»è¾‘
    processed_data = process_data_jun_beijing(contract_data, existing_contract_ids,housekeeper_award_lists)
    logging.info('BEIJING 2025 7æœˆ, Data processed')

    performance_data_headers = ['æ´»åŠ¨ç¼–å·', 'åˆåŒID(_id)', 'æ´»åŠ¨åŸå¸‚(province)', 'å·¥å•ç¼–å·(serviceAppointmentNum)', 'Status', 'ç®¡å®¶(serviceHousekeeper)', 'åˆåŒç¼–å·(contractdocNum)', 'åˆåŒé‡‘é¢(adjustRefundMoney)', 'æ”¯ä»˜é‡‘é¢(paidAmount)', 'å·®é¢(difference)', 'State', 'åˆ›å»ºæ—¶é—´(createTime)', 'æœåŠ¡å•†(orgName)', 'ç­¾çº¦æ—¶é—´(signedDate)', 'Doorsill', 'æ¬¾é¡¹æ¥æºç±»å‹(tradeIn)', 'è½¬åŒ–ç‡(conversion)', 'å¹³å‡å®¢å•ä»·(average)','æ´»åŠ¨æœŸå†…ç¬¬å‡ ä¸ªåˆåŒ','ç®¡å®¶ç´¯è®¡é‡‘é¢','ç®¡å®¶ç´¯è®¡å•æ•°','å¥–é‡‘æ± ','è®¡å…¥ä¸šç»©é‡‘é¢','æ¿€æ´»å¥–åŠ±çŠ¶æ€', 'å¥–åŠ±ç±»å‹', 'å¥–åŠ±åç§°', 'æ˜¯å¦å‘é€é€šçŸ¥', 'å¤‡æ³¨', 'ç™»è®°æ—¶é—´']

    write_performance_data(performance_data_filename, processed_data, performance_data_headers)

    # å½“æœˆçš„æ•°æ®å¤„ç†é€»è¾‘
    notify_awards_jun_beijing(performance_data_filename, status_filename)

    archive_file(contract_data_filename)
    logging.info('BEIJING 2025 7æœˆ, Data archived')

    logging.info('BEIJING 2025 7æœˆ, Job ended')
    
# 2025å¹´6æœˆï¼ŒåŒ—äº¬. 
# å¹¸è¿æ•°å­—8ï¼Œå•åˆåŒé‡‘é¢1ä¸‡ä»¥ä¸Šå’Œä»¥ä¸‹å¹¸è¿å¥–åŠ±ä¸åŒï¼›èŠ‚èŠ‚é«˜ä¸‰æ¡£ï¼›
# å•ä¸ªé¡¹ç›®ï¼ˆå·¥å•ï¼‰ç­¾çº¦åˆåŒé‡‘é¢å¤§äº5ä¸‡æ—¶ï¼Œå‚ä¸ç´¯è®¡åˆåŒé‡‘é¢è®¡ç®—æ—¶å‡æŒ‰5ä¸‡è®¡å…¥ã€‚
def signing_and_sales_incentive_jun_beijing():
    contract_data_filename = TEMP_CONTRACT_DATA_FILE_BJ_JUN
    performance_data_filename = PERFORMANCE_DATA_FILENAME_BJ_JUN
    status_filename = STATUS_FILENAME_BJ_JUN
    api_url = API_URL_BJ_JUN

    logging.info('BEIJING 2025 6æœˆ, Job started ...')

    response = send_request_with_managed_session(api_url)
 
    logging.info('BEIJING 2025 6æœˆ, Request sent')

    rows = response['data']['rows']

    columns = ["åˆåŒID(_id)", "æ´»åŠ¨åŸå¸‚(province)", "å·¥å•ç¼–å·(serviceAppointmentNum)", "Status", "ç®¡å®¶(serviceHousekeeper)", "åˆåŒç¼–å·(contractdocNum)", "åˆåŒé‡‘é¢(adjustRefundMoney)", "æ”¯ä»˜é‡‘é¢(paidAmount)", "å·®é¢(difference)", "State", "åˆ›å»ºæ—¶é—´(createTime)", "æœåŠ¡å•†(orgName)", "ç­¾çº¦æ—¶é—´(signedDate)", "Doorsill", "æ¬¾é¡¹æ¥æºç±»å‹(tradeIn)", "è½¬åŒ–ç‡(conversion)", "å¹³å‡å®¢å•ä»·(average)"]
    save_to_csv_with_headers(rows,contract_data_filename,columns)

    logging.info(f'BEIJING 2025 6æœˆ, Data saved to {contract_data_filename}')

    contract_data = read_contract_data(contract_data_filename)

    existing_contract_ids = collect_unique_contract_ids_from_file(performance_data_filename)

    housekeeper_award_lists = get_housekeeper_award_list(performance_data_filename)

    # å½“æœˆçš„æ•°æ®å¤„ç†é€»è¾‘
    processed_data = process_data_jun_beijing(contract_data, existing_contract_ids,housekeeper_award_lists)
    logging.info('BEIJING 2025 6æœˆ, Data processed')

    performance_data_headers = ['æ´»åŠ¨ç¼–å·', 'åˆåŒID(_id)', 'æ´»åŠ¨åŸå¸‚(province)', 'å·¥å•ç¼–å·(serviceAppointmentNum)', 'Status', 'ç®¡å®¶(serviceHousekeeper)', 'åˆåŒç¼–å·(contractdocNum)', 'åˆåŒé‡‘é¢(adjustRefundMoney)', 'æ”¯ä»˜é‡‘é¢(paidAmount)', 'å·®é¢(difference)', 'State', 'åˆ›å»ºæ—¶é—´(createTime)', 'æœåŠ¡å•†(orgName)', 'ç­¾çº¦æ—¶é—´(signedDate)', 'Doorsill', 'æ¬¾é¡¹æ¥æºç±»å‹(tradeIn)', 'è½¬åŒ–ç‡(conversion)', 'å¹³å‡å®¢å•ä»·(average)','æ´»åŠ¨æœŸå†…ç¬¬å‡ ä¸ªåˆåŒ','ç®¡å®¶ç´¯è®¡é‡‘é¢','ç®¡å®¶ç´¯è®¡å•æ•°','å¥–é‡‘æ± ','è®¡å…¥ä¸šç»©é‡‘é¢','æ¿€æ´»å¥–åŠ±çŠ¶æ€', 'å¥–åŠ±ç±»å‹', 'å¥–åŠ±åç§°', 'æ˜¯å¦å‘é€é€šçŸ¥', 'å¤‡æ³¨', 'ç™»è®°æ—¶é—´']

    write_performance_data(performance_data_filename, processed_data, performance_data_headers)

    # å½“æœˆçš„æ•°æ®å¤„ç†é€»è¾‘
    notify_awards_jun_beijing(performance_data_filename, status_filename)

    archive_file(contract_data_filename)
    logging.info('BEIJING 2025 6æœˆ, Data archived')

    logging.info('BEIJING 2025 6æœˆ, Job ended')

# 2025å¹´7æœˆï¼Œä¸Šæµ·. ç­¾çº¦å’Œå¥–åŠ±æ’­æŠ¥ï¼Œè§„åˆ™ä¸4æœˆç›¸åŒ
def signing_and_sales_incentive_july_shanghai():
    contract_data_filename = TEMP_CONTRACT_DATA_FILE_SH_JULY
    performance_data_filename = PERFORMANCE_DATA_FILENAME_SH_JULY
    status_filename = STATUS_FILENAME_SH_JULY
    api_url = API_URL_SH_JULY

    logging.info('SHANGHAI 2025 7æœˆ Conq & triumph, take 1 more city, Job started ...')
    response = send_request_with_managed_session(api_url)
    logging.info('SHANGHAI 2025 7æœˆ Conq & triumph, take 1 more city, Request sent')

    rows = response['data']['rows']

    columns = ["åˆåŒID(_id)", "æ´»åŠ¨åŸå¸‚(province)", "å·¥å•ç¼–å·(serviceAppointmentNum)", "Status", "ç®¡å®¶(serviceHousekeeper)", "åˆåŒç¼–å·(contractdocNum)", "åˆåŒé‡‘é¢(adjustRefundMoney)", "æ”¯ä»˜é‡‘é¢(paidAmount)", "å·®é¢(difference)", "State", "åˆ›å»ºæ—¶é—´(createTime)", "æœåŠ¡å•†(orgName)", "ç­¾çº¦æ—¶é—´(signedDate)", "Doorsill", "æ¬¾é¡¹æ¥æºç±»å‹(tradeIn)", "è½¬åŒ–ç‡(conversion)", "å¹³å‡å®¢å•ä»·(average)"]
    save_to_csv_with_headers(rows,contract_data_filename,columns)

    logging.info(f'SHANGHAI 2025 7æœˆ Conq & triumph, take 1 more city, Data saved to {contract_data_filename}')

    contract_data = read_contract_data(contract_data_filename)

    existing_contract_ids = collect_unique_contract_ids_from_file(performance_data_filename)

    # è·å–ç®¡å®¶å¥–åŠ±åˆ—è¡¨ï¼Œå‡çº§å”¯ä¸€å¥–åŠ±åˆ—è¡¨
    housekeeper_award_lists = get_unique_housekeeper_award_list(performance_data_filename)

    # å½“æœˆçš„æ•°æ®å¤„ç†é€»è¾‘ï¼Œå¥–åŠ±è§„åˆ™ä¸4æœˆä¿æŒä¸€è‡´
    processed_data = process_data_shanghai_apr(contract_data, existing_contract_ids, housekeeper_award_lists)

    logging.info('SHANGHAI 2025 7æœˆ Conq & triumph, take 1 more city, Data processed')

    performance_data_headers = ['æ´»åŠ¨ç¼–å·', 'åˆåŒID(_id)', 'æ´»åŠ¨åŸå¸‚(province)', 'å·¥å•ç¼–å·(serviceAppointmentNum)', 'Status', 'ç®¡å®¶(serviceHousekeeper)', 'åˆåŒç¼–å·(contractdocNum)', 'åˆåŒé‡‘é¢(adjustRefundMoney)', 'æ”¯ä»˜é‡‘é¢(paidAmount)', 'å·®é¢(difference)', 'State', 'åˆ›å»ºæ—¶é—´(createTime)', 'æœåŠ¡å•†(orgName)', 'ç­¾çº¦æ—¶é—´(signedDate)', 'Doorsill', 'æ¬¾é¡¹æ¥æºç±»å‹(tradeIn)', 'è½¬åŒ–ç‡(conversion)', 'å¹³å‡å®¢å•ä»·(average)','æ´»åŠ¨æœŸå†…ç¬¬å‡ ä¸ªåˆåŒ','ç®¡å®¶ç´¯è®¡é‡‘é¢','ç®¡å®¶ç´¯è®¡å•æ•°','å¥–é‡‘æ± ', 'è®¡å…¥ä¸šç»©é‡‘é¢','æ¿€æ´»å¥–åŠ±çŠ¶æ€', 'å¥–åŠ±ç±»å‹', 'å¥–åŠ±åç§°', 'æ˜¯å¦å‘é€é€šçŸ¥', 'å¤‡æ³¨', 'ç™»è®°æ—¶é—´']

    write_performance_data(performance_data_filename, processed_data, performance_data_headers)

    # å½“æœˆçš„é€šçŸ¥æ•°æ®å¤„ç†é€»è¾‘ï¼ˆä¸ä¸‰æœˆä¸€è‡´ï¼‰ï¼Œä¸4æœˆä¿æŒä¸€è‡´
    notify_awards_shanghai_generate_message_march(performance_data_filename, status_filename, contract_data)

    archive_file(contract_data_filename)
    logging.info('SHANGHAI 2025 7æœˆ Conq & triumph, take 1 more city, Data archived')

    logging.info('SHANGHAI 2025 7æœˆ Conq & triumph, take 1 more city, Job ended')   

# 2025å¹´6æœˆï¼Œä¸Šæµ·. ç­¾çº¦å’Œå¥–åŠ±æ’­æŠ¥ï¼Œè§„åˆ™ä¸4æœˆç›¸åŒ
def signing_and_sales_incentive_jun_shanghai():
    contract_data_filename = TEMP_CONTRACT_DATA_FILE_SH_JUN
    performance_data_filename = PERFORMANCE_DATA_FILENAME_SH_JUN
    status_filename = STATUS_FILENAME_SH_JUN
    api_url = API_URL_SH_JUN

    logging.info('SHANGHAI 2025 6æœˆ Conq & triumph, take 1 more city, Job started ...')
    response = send_request_with_managed_session(api_url)
    logging.info('SHANGHAI 2025 6æœˆ Conq & triumph, take 1 more city, Request sent')

    rows = response['data']['rows']

    columns = ["åˆåŒID(_id)", "æ´»åŠ¨åŸå¸‚(province)", "å·¥å•ç¼–å·(serviceAppointmentNum)", "Status", "ç®¡å®¶(serviceHousekeeper)", "åˆåŒç¼–å·(contractdocNum)", "åˆåŒé‡‘é¢(adjustRefundMoney)", "æ”¯ä»˜é‡‘é¢(paidAmount)", "å·®é¢(difference)", "State", "åˆ›å»ºæ—¶é—´(createTime)", "æœåŠ¡å•†(orgName)", "ç­¾çº¦æ—¶é—´(signedDate)", "Doorsill", "æ¬¾é¡¹æ¥æºç±»å‹(tradeIn)", "è½¬åŒ–ç‡(conversion)", "å¹³å‡å®¢å•ä»·(average)"]
    save_to_csv_with_headers(rows,contract_data_filename,columns)

    logging.info(f'SHANGHAI 2025 6æœˆ Conq & triumph, take 1 more city, Data saved to {contract_data_filename}')

    contract_data = read_contract_data(contract_data_filename)

    existing_contract_ids = collect_unique_contract_ids_from_file(performance_data_filename)

    # è·å–ç®¡å®¶å¥–åŠ±åˆ—è¡¨ï¼Œå‡çº§å”¯ä¸€å¥–åŠ±åˆ—è¡¨
    housekeeper_award_lists = get_unique_housekeeper_award_list(performance_data_filename)

    # å½“æœˆçš„æ•°æ®å¤„ç†é€»è¾‘ï¼Œå¥–åŠ±è§„åˆ™ä¸4æœˆä¿æŒä¸€è‡´
    processed_data = process_data_shanghai_apr(contract_data, existing_contract_ids, housekeeper_award_lists)

    logging.info('SHANGHAI 2025 6æœˆ Conq & triumph, take 1 more city, Data processed')

    performance_data_headers = ['æ´»åŠ¨ç¼–å·', 'åˆåŒID(_id)', 'æ´»åŠ¨åŸå¸‚(province)', 'å·¥å•ç¼–å·(serviceAppointmentNum)', 'Status', 'ç®¡å®¶(serviceHousekeeper)', 'åˆåŒç¼–å·(contractdocNum)', 'åˆåŒé‡‘é¢(adjustRefundMoney)', 'æ”¯ä»˜é‡‘é¢(paidAmount)', 'å·®é¢(difference)', 'State', 'åˆ›å»ºæ—¶é—´(createTime)', 'æœåŠ¡å•†(orgName)', 'ç­¾çº¦æ—¶é—´(signedDate)', 'Doorsill', 'æ¬¾é¡¹æ¥æºç±»å‹(tradeIn)', 'è½¬åŒ–ç‡(conversion)', 'å¹³å‡å®¢å•ä»·(average)','æ´»åŠ¨æœŸå†…ç¬¬å‡ ä¸ªåˆåŒ','ç®¡å®¶ç´¯è®¡é‡‘é¢','ç®¡å®¶ç´¯è®¡å•æ•°','å¥–é‡‘æ± ', 'è®¡å…¥ä¸šç»©é‡‘é¢','æ¿€æ´»å¥–åŠ±çŠ¶æ€', 'å¥–åŠ±ç±»å‹', 'å¥–åŠ±åç§°', 'æ˜¯å¦å‘é€é€šçŸ¥', 'å¤‡æ³¨', 'ç™»è®°æ—¶é—´']

    write_performance_data(performance_data_filename, processed_data, performance_data_headers)

    # å½“æœˆçš„é€šçŸ¥æ•°æ®å¤„ç†é€»è¾‘ï¼ˆä¸ä¸‰æœˆä¸€è‡´ï¼‰ï¼Œä¸4æœˆä¿æŒä¸€è‡´
    notify_awards_shanghai_generate_message_march(performance_data_filename, status_filename, contract_data)

    archive_file(contract_data_filename)
    logging.info('SHANGHAI 2025 6æœˆ Conq & triumph, take 1 more city, Data archived')

    logging.info('SHANGHAI 2025 6æœˆ Conq & triumph, take 1 more city, Job ended')   

def check_technician_status():
    api_url = API_URL_TS
    status_filename = STATUS_FILENAME_TS

    logging.info('BEIJING, Technician Status Check Job started')

    response = send_request_with_managed_session(api_url)    
    status_changes = response['data']['rows']

    notify_technician_status_changes(status_changes, status_filename)

    logging.info('BEIJING, Technician Status Check Job ended') 

def generate_daily_service_report():
    logging.info('Daily service report generation started...')
    api_url = API_URL_DAILY_SERVICE_REPORT
    temp_daily_service_report_file = TEMP_DAILY_SERVICE_REPORT_FILE
    status_code_filename = DAILY_SERVICE_REPORT_RECORD_FILE

    try:
        # 1. å‘é€è¯·æ±‚ä»¥è·å–æ—¥æŠ¥æ•°æ®
        response = send_request_with_managed_session(api_url)
        logging.info('Daily service report request sent successfully.')

        # 2. å¤„ç†å“åº”æ•°æ®
        report_data = response['data']['rows']
        if not report_data:
            logging.warning('No data found for the daily service report.')
            # return

        # 3. ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶
        columns = ["_id", "sid", "saCreateTime", "orderNum", "province", "orgName", "supervisorName", "sourceType", "status", "msg", "memo", "workType", "createTime"]
        save_to_csv_with_headers(report_data, temp_daily_service_report_file, columns)

        # 4. è¯»å–æ•°æ®
        report_data = read_daily_service_report(temp_daily_service_report_file)
        logging.info(f"Report data: {report_data}")

        # æ–°çš„SLAè¿è§„æ£€æŸ¥å¹¶å‘é€é€šçŸ¥æœåŠ¡
        process_sla_violations(report_data)
        logging.info('SLA violations processed successfully.')

        # # å½“å‰é€‚ç”¨çš„å‘é€æ—¥å¸¸æœåŠ¡æŠ¥å‘Š
        # notify_daily_service_report(report_data, status_code_filename)
        # logging.info('Daily service report notification sent successfully.')

    except Exception as e:
        logging.error(f"An error occurred: {e}")

    logging.info('Daily service report generation completed.')

def check_contact_timeout():
    api_url = API_URL_CONTACT_TIMEOUT
    # notify_status_filename = STATUS_FILENAME_CONTACT_TIMEOUT

    logging.info('Contact Timeout Check, Job started ...')

    response = send_request_with_managed_session(api_url)

    if response is None:
        logging.error('Failed to get response for contact timeout check')
        return

    contact_timeout_data = response['data']['rows']
    print(contact_timeout_data)  # æ‰“å° status_changes

    notify_contact_timeout_changes_template_card(contact_timeout_data)

    logging.info('Contact Timeout Check, Job ended')

def format_create_time(iso_time_str):
    """å°†ISOæ—¶é—´æ ¼å¼è½¬æ¢ä¸ºæ˜“è¯»æ ¼å¼"""
    from datetime import datetime
    try:
        # å¤„ç†å¸¦æ—¶åŒºçš„ISOæ ¼å¼
        if '+' in iso_time_str:
            dt = datetime.fromisoformat(iso_time_str)
        else:
            dt = datetime.fromisoformat(iso_time_str.replace('Z', '+00:00'))
        return dt.strftime('%Y-%m-%d %H:%M')
    except Exception as e:
        logging.warning(f'æ—¶é—´æ ¼å¼è½¬æ¢å¤±è´¥: {iso_time_str}, é”™è¯¯: {e}')
        return iso_time_str

def filter_orders_by_time_threshold(orders_data):
    """
    è¿‡æ»¤å·¥å•æ•°æ®ï¼Œæ’é™¤ï¼š
    - å¾…é¢„çº¦çŠ¶æ€ï¼Œ48å°æ—¶ä¹‹å†…çš„éœ€è¦æ’é™¤
    - æš‚ä¸ä¸Šé—¨çŠ¶æ€ï¼Œ48å°æ—¶ä¹‹å†…çš„éœ€è¦æ’é™¤

    Args:
        orders_data: åŸå§‹å·¥å•æ•°æ®åˆ—è¡¨

    Returns:
        filtered_orders: è¿‡æ»¤åçš„å·¥å•æ•°æ®åˆ—è¡¨
    """
    from datetime import datetime, timezone

    filtered_orders = []
    current_time = datetime.now(timezone.utc)

    for order in orders_data:
        try:
            # è§£æå·¥å•æ•°æ®
            order_info = {
                'orderNum': order[0],
                'name': order[1],
                'address': order[2],
                'supervisorName': order[3],
                'createTime': order[4],
                'orgName': order[5],
                'orderstatus': order[6]
            }

            # è§£æåˆ›å»ºæ—¶é—´
            create_time_str = order_info['createTime']
            if '+' in create_time_str:
                create_time = datetime.fromisoformat(create_time_str)
            else:
                create_time = datetime.fromisoformat(create_time_str.replace('Z', '+00:00'))

            # ç¡®ä¿åˆ›å»ºæ—¶é—´æœ‰æ—¶åŒºä¿¡æ¯
            if create_time.tzinfo is None:
                create_time = create_time.replace(tzinfo=timezone.utc)

            # è®¡ç®—æ—¶é—´å·®ï¼ˆå°æ—¶ï¼‰
            time_diff = current_time - create_time
            hours_elapsed = time_diff.total_seconds() / 3600

            # è·å–å·¥å•çŠ¶æ€
            order_status = order_info['orderstatus']

            # è¿‡æ»¤é€»è¾‘ï¼šå¾…é¢„çº¦å’Œæš‚ä¸ä¸Šé—¨çŠ¶æ€ï¼Œ48å°æ—¶ä¹‹å†…çš„éœ€è¦æ’é™¤
            should_include = True

            if ('å¾…é¢„çº¦' in order_status or 'æš‚ä¸ä¸Šé—¨' in order_status) and hours_elapsed < 48:
                should_include = False
                logging.info(f"è¿‡æ»¤æ‰å·¥å• {order_info['orderNum']} (çŠ¶æ€: {order_status}, åˆ›å»ºæ—¶é—´: {hours_elapsed:.1f}å°æ—¶å‰)")

            if should_include:
                filtered_orders.append(order)

        except Exception as e:
            logging.warning(f"å¤„ç†å·¥å•æ•°æ®æ—¶å‡ºé”™ï¼Œè·³è¿‡: {order}, é”™è¯¯: {e}")
            continue

    return filtered_orders

def group_orders_by_org(orders_data):
    """æŒ‰æœåŠ¡å•†åˆ†ç»„å·¥å•æ•°æ®"""
    grouped = {}

    for order in orders_data:
        try:
            # æ ¹æ®APIæµ‹è¯•ç»“æœï¼Œå­—æ®µç´¢å¼•æ˜ å°„
            order_info = {
                'orderNum': order[0],
                'name': order[1],
                'address': order[2],
                'supervisorName': order[3],
                'createTime': order[4],
                'orgName': order[5],
                'orderstatus': order[6]
            }

            org_name = order_info['orgName']
            if org_name not in grouped:
                grouped[org_name] = []
            grouped[org_name].append(order_info)

        except (IndexError, KeyError) as e:
            logging.warning(f'å·¥å•æ•°æ®æ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡: {order}, é”™è¯¯: {e}')
            continue

    return grouped

def format_pending_orders_message_text(org_name, orders):
    """æ ¼å¼åŒ–å·¥å•æé†’æ¶ˆæ¯ï¼ˆæ–‡æœ¬æ ¼å¼ï¼Œä¿ç•™ä½œä¸ºå¤‡ç”¨ï¼‰"""
    count = len(orders)

    message_lines = [
        f"ğŸ“‹ å¾…é¢„çº¦å·¥å•æé†’ ({org_name})",
        "",
        f"å…±æœ‰ {count} ä¸ªå·¥å•å¾…é¢„çº¦ï¼š",
        ""
    ]

    for i, order in enumerate(orders, 1):
        # ä½¿ç”¨æ–°çš„ç®€åŒ–æ ¼å¼
        simple_date = format_simple_date(order['createTime'])
        retention_duration = calculate_retention_duration(order['createTime'])
        simple_order_num = simplify_order_number(order['orderNum'])

        order_text = f"""{i:02d}. å·¥å•å·ï¼š{simple_order_num}
     å®¢æˆ·ï¼š{order['name']}
     åœ°å€ï¼š{order['address']}
     è´Ÿè´£äººï¼š{order['supervisorName']}
     åˆ›å»ºæ—¶é—´ï¼š{simple_date}ï¼ˆ{retention_duration}ï¼‰
     çŠ¶æ€ï¼š{order['orderstatus']}"""

        message_lines.append(order_text)
        if i < count:  # ä¸æ˜¯æœ€åä¸€ä¸ªï¼Œæ·»åŠ ç©ºè¡Œ
            message_lines.append("")

    message_lines.extend([
        "",
        "è¯·åŠæ—¶è·Ÿè¿›å¤„ç†ï¼Œå¦‚æœ‰ç–‘é—®è¯·è”ç³»è¿è¥äººå‘˜ã€‚"
    ])

    return "\n".join(message_lines)

def simplify_order_number(order_num):
    """ç®€åŒ–å·¥å•å·ï¼Œåªä¿ç•™å5ä½æ•°å­—"""
    if not order_num:
        return "-"

    # æå–æ•°å­—éƒ¨åˆ†
    import re
    numbers = re.findall(r'\d+', order_num)
    if numbers:
        # å–æœ€åä¸€ä¸ªæ•°å­—ä¸²çš„å5ä½
        last_number = numbers[-1]
        if len(last_number) >= 5:
            return last_number[-5:]
        else:
            return last_number
    return order_num

def format_simple_date(create_time_str):
    """æ ¼å¼åŒ–åˆ›å»ºæ—¶é—´ä¸ºç®€å•çš„æœˆ-æ—¥æ ¼å¼"""
    from datetime import datetime

    try:
        # è§£æåˆ›å»ºæ—¶é—´
        if '+' in create_time_str:
            create_time = datetime.fromisoformat(create_time_str)
        else:
            create_time = datetime.fromisoformat(create_time_str.replace('Z', '+00:00'))

        # æ ¼å¼åŒ–ä¸ºMM-DD
        return f"{create_time.month:02d}-{create_time.day:02d}"

    except Exception as e:
        logging.warning(f"æ—¶é—´æ ¼å¼åŒ–å¤±è´¥: {create_time_str}, é”™è¯¯: {e}")
        return "æœªçŸ¥"

def calculate_retention_duration(create_time_str):
    """è®¡ç®—å·¥å•æ»ç•™æ—¶é•¿"""
    from datetime import datetime, timezone

    try:
        # è§£æåˆ›å»ºæ—¶é—´
        if '+' in create_time_str:
            create_time = datetime.fromisoformat(create_time_str)
        else:
            create_time = datetime.fromisoformat(create_time_str.replace('Z', '+00:00'))

        # è·å–å½“å‰æ—¶é—´ï¼ˆå¸¦æ—¶åŒºï¼‰
        current_time = datetime.now(timezone.utc)

        # ç¡®ä¿åˆ›å»ºæ—¶é—´ä¹Ÿæœ‰æ—¶åŒºä¿¡æ¯
        if create_time.tzinfo is None:
            create_time = create_time.replace(tzinfo=timezone.utc)

        # è®¡ç®—æ—¶é—´å·®
        duration = current_time - create_time

        # æ ¼å¼åŒ–æ˜¾ç¤ºï¼ˆç®€åŒ–ä¸ºå¤©æ•°é¢—ç²’åº¦ï¼‰
        days = int(duration.total_seconds() // (24 * 3600))
        return f"{days}å¤©"

    except Exception as e:
        logging.warning(f"æ»ç•™æ—¶é•¿è®¡ç®—å¤±è´¥: {create_time_str}, é”™è¯¯: {e}")
        return "æœªçŸ¥"

def format_pending_orders_message(org_name, orders):
    """æ ¼å¼åŒ–å·¥å•æé†’æ¶ˆæ¯ï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰"""
    count = len(orders)

    # æ¶ˆæ¯å¤´éƒ¨
    header = f"ğŸ“‹ **å¾…é¢„çº¦å·¥å•æé†’** ({org_name})\n\nå…±æœ‰ **{count}** ä¸ªå·¥å•å¾…é¢„çº¦ï¼š\n"

    # è¡¨æ ¼å¤´éƒ¨ï¼ˆå¢åŠ æ»ç•™æ—¶é•¿åˆ—ï¼‰
    table_header = """
| åºå· | å·¥å•å· | æ»ç•™æ—¶é•¿ | å®¢æˆ· | åœ°å€ | ç®¡å®¶ | åˆ›å»ºæ—¶é—´ | çŠ¶æ€ |
| --- | --- | --- | --- | --- | --- | --- | --- |"""

    # è¡¨æ ¼å†…å®¹
    table_rows = []
    for i, order in enumerate(orders, 1):
        # ä½¿ç”¨æ–°çš„ç®€åŒ–æ ¼å¼
        simple_date = format_simple_date(order['createTime'])
        retention_duration = calculate_retention_duration(order['createTime'])
        simple_order_num = simplify_order_number(order['orderNum'])

        # å¤„ç†å¯èƒ½åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„å­—æ®µ
        customer = order['name'].replace('|', '\\|') if order['name'] else '-'
        address = order['address'].replace('|', '\\|') if order['address'] else '-'
        supervisor = order['supervisorName'].replace('|', '\\|') if order['supervisorName'] else '-'
        status = order['orderstatus'].replace('|', '\\|') if order['orderstatus'] else '-'

        row = f"| {i:02d} | {simple_order_num} | {retention_duration} | {customer} | {address} | {supervisor} | {simple_date} | {status} |"
        table_rows.append(row)

    # æ¶ˆæ¯åº•éƒ¨
    footer = "\n\nè¯·åŠæ—¶è·Ÿè¿›å¤„ç†ï¼Œå¦‚æœ‰ç–‘é—®è¯·è”ç³»è¿è¥äººå‘˜ã€‚"

    # ç»„åˆå®Œæ•´æ¶ˆæ¯
    message = header + table_header + "\n" + "\n".join(table_rows) + footer

    return message

def send_pending_orders_reminder():
    """å¾…é¢„çº¦å·¥å•æé†’ä»»åŠ¡"""
    from datetime import datetime

    logging.info('å¾…é¢„çº¦å·¥å•æé†’ä»»åŠ¡å¼€å§‹...')

    try:
        # 1. è·å–æ•°æ®
        api_url = API_URL_PENDING_ORDERS_REMINDER
        logging.info('æ­£åœ¨è·å–å¾…é¢„çº¦å·¥å•æ•°æ®...')
        response = send_request_with_managed_session(api_url)

        if not response or 'data' not in response:
            logging.error('APIè¯·æ±‚å¤±è´¥æˆ–æ•°æ®æ ¼å¼å¼‚å¸¸')
            return

        orders_data = response['data']['rows']
        total_orders = len(orders_data)
        logging.info(f'è·å–åˆ° {total_orders} æ¡åŸå§‹å·¥å•æ•°æ®')

        if total_orders == 0:
            logging.info('æ²¡æœ‰å¾…é¢„çº¦å·¥å•ï¼Œä»»åŠ¡ç»“æŸ')
            return

        # 2. åº”ç”¨æ—¶é—´è¿‡æ»¤
        logging.info('æ­£åœ¨åº”ç”¨æ—¶é—´è¿‡æ»¤è§„åˆ™...')
        logging.info('- æ’é™¤å¾…é¢„çº¦çŠ¶æ€48å°æ—¶ä¹‹å†…çš„å·¥å•')
        logging.info('- æ’é™¤æš‚ä¸ä¸Šé—¨çŠ¶æ€48å°æ—¶ä¹‹å†…çš„å·¥å•')
        filtered_orders_data = filter_orders_by_time_threshold(orders_data)
        filtered_count = len(filtered_orders_data)
        logging.info(f'è¿‡æ»¤åå‰©ä½™ {filtered_count} æ¡å·¥å•æ•°æ®')

        if filtered_count == 0:
            logging.info('è¿‡æ»¤åæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„å·¥å•ï¼Œä»»åŠ¡ç»“æŸ')
            return

        # 3. æ•°æ®å¤„ç†å’Œåˆ†ç»„
        logging.info('æ­£åœ¨æŒ‰æœåŠ¡å•†åˆ†ç»„å·¥å•æ•°æ®...')
        grouped_orders = group_orders_by_org(filtered_orders_data)
        org_count = len(grouped_orders)
        logging.info(f'å…±åˆ†ä¸º {org_count} ä¸ªæœåŠ¡å•†ç»„')

        # 4. å‘é€é€šçŸ¥
        success_count = 0
        failed_count = 0

        for org_name, orders in grouped_orders.items():
            try:
                logging.info(f'æ­£åœ¨ä¸º {org_name} å‘é€æé†’ï¼Œå·¥å•æ•°é‡: {len(orders)}')

                # æ ¼å¼åŒ–æ¶ˆæ¯ï¼ˆä½¿ç”¨æ–‡å­—ç‰ˆæ ¼å¼ï¼‰
                message = format_pending_orders_message_text(org_name, orders)

                # è·å–webhookåœ°å€
                webhook_url = ORG_WEBHOOKS.get(org_name, WEBHOOK_URL_DEFAULT)

                # å‘é€æ¶ˆæ¯ï¼ˆä½¿ç”¨æ–‡å­—æ ¼å¼ï¼‰
                post_text_to_webhook(message, webhook_url)

                success_count += 1
                logging.info(f'âœ“ {org_name} æé†’å‘é€æˆåŠŸ')

            except Exception as e:
                failed_count += 1
                logging.error(f'âœ— {org_name} æé†’å‘é€å¤±è´¥: {e}')

        # 5. ä»»åŠ¡æ€»ç»“
        logging.info(f'å¾…é¢„çº¦å·¥å•æé†’ä»»åŠ¡å®Œæˆ - æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}')

    except Exception as e:
        logging.error(f'å¾…é¢„çº¦å·¥å•æé†’ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}')
        import traceback
        logging.error(traceback.format_exc())
    